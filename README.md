# histogram.py

## ðŸ“Š But

`histogram.py` est un script d'exploration (EDA - Exploratory Data Analysis) qui affiche des histogrammes des notes pour un cours donnÃ© (feature numÃ©rique), sÃ©parÃ©es par maison (Gryffindor, Hufflepuff, Ravenclaw, Slytherin), afin de rÃ©pondre Ã  la question du sujet : **quel cours a une distribution de scores la plus homogÃ¨ne entre les quatre maisons**.

### ðŸŽ¨ Couleurs des maisons

- **Gryffindor** = brown (marron)
- **Hufflepuff** = red (rouge)
- **Ravenclaw** = yellow (jaune)
- **Slytherin** = green (vert)

## ðŸ’¡ Pourquoi c'est utile

Cette visualisation permet d'identifier :
- Les cours **peu discriminants** (distributions trÃ¨s similaires entre maisons)
- Les cours qui **sÃ©parent mieux les maisons**

Ces informations permettent de choisir les features pertinentes pour entraÃ®ner la rÃ©gression logistique et dÃ©cider quel Ã©lÃ¨ve devra aller dans quelle maison.

## ðŸ“¥ EntrÃ©e / Sortie

### EntrÃ©e
- `dataset_train.csv` (le jeu d'entraÃ®nement)

### Sortie
- Une fenÃªtre de plot (matplotlib) avec l'histogramme comparatif par maison
- Le script ignore les valeurs manquantes pour ne pas biaiser la distribution

## ðŸš€ Exemples de lancement

```bash
python histogram.py datasets/dataset_train.csv
```

```bash
python histogram.py datasets/dataset_train.csv "Astronomy"
```

Ou via le Makefile :

```bash
make histogram
```

# Log Train

Etape 1: 
    Transformer Maison en nombre en appliquant la tech du One For All:
        ex: Gryffundor = 1;
        Hufflepuff = Slytherin = Ravenclaw = 0;

Etape 2:
Utilisation du Machine Learning
Faire recherche du Poids W et du Biais B de la fonction Logistic Regression: z = w * x + b:
Pour Cela, entrainer le modele;
Repeter l'etape jusqu'a difference minime

Pour cela:
    
âˆ‚L / âˆ‚w â€‹= 1/n * â€‹âˆ‘( y^â€‹ âˆ’ y )â‹…x;
w= w âˆ’ Î± * âˆ‚L / âˆ‚w;

âˆ‚L / âˆ‚b â€‹= 1/n * â€‹âˆ‘( y^â€‹âˆ’y)
b = b âˆ’ Î± * âˆ‚L / âˆ‚b

Repeter l'etape jusqu'a difference faible entre les deux W et les deux B


Etape 3?:
Lâ€™algorithme : One vs All


# Lien Utile

https://mrmint.fr/gradient-descent-algorithm
https://www.geeksforgeeks.org/machine-learning/derivative-of-the-sigmoid-function/

